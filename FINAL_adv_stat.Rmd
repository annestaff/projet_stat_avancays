---
title: "Advanced statistic"
author: "Anne STAFF, Bianca BOI"
output:
  html_document:
    df_print: paged
---



<br>
<br>
<br>

# Description of the project
<br>
<br>
• in the first part, you will obtain the maximum likelihood estimator for the Log-logistic distribution
and implement a Newton-Raphson algorithm. 
<br>



• in the second part, you will study the real data set using descriptive statistics and an R-package for
maximum likelihood estimation. You will also obtain bootstrap-based confidence intervals.

<br>
<br>
<br>

# Maximum Likelihood estimation
<br>
<br>


We are interested in estimating the parameters of a Log-logistic distribution. $X \sim LL(\alpha , \beta)$ if X has a probability density function :
$$f_X (x) = \frac{(\frac{\beta}{\alpha})(\frac{x}{\alpha})^{\beta-1}}{[1+(\frac{x}{\alpha})^\beta]^2} , x,\alpha,\beta > 0$$
We consider an i.i.d n-sample $(X_1,...,X_n)$ where the $X_i$'s arise from a Log-logistic distribution subject to random right censoring.
<br>
<br>
<br>

## Corresponding statistical model
<br>
<br>

The corresponding statistical model for the given data, which follows a log-logistic distribution subject to random right censoring, is the censored log-logistic regression model.

This model allows for the estimation of the parameters of the log-logistic distribution, taking into account the censoring of some observations at a pre-specified threshold value.


The density probability is : $$f_X (x_i; \alpha,\beta) = \frac{(\frac{\beta}{\alpha})(\frac{x_i}{\alpha})^{\beta-1}}{[1+(\frac{x_i}{\alpha})^\beta]^2} , x,\alpha,\beta > 0$$
The cumulative distribution function is the following : 
$$F(X_i; \alpha,\beta) = \frac{1}{1 +(\frac{X_i}{\alpha})^\beta} $$

The non-parametric survival function $S(t;  \alpha,\beta)$ is given by:

$$S(t; \alpha,\beta) = 1-F(t) = [1 + (\frac{t}{\alpha})^\beta]^-1$$

where t is the time, and $\alpha$ and $\beta$ are the parameters of the log-logistic distribution.



$X \sim LL(\alpha , \beta)$ and we consider an i.i.d n-sample $(X_1,...,X_n)$ where the $X_i$'s arise from a Log-logistic distribution so the sampling hypothesis is admitted. 

The c.d.f of X belongs well to a parametric family $F$, such that : 
$$F = \{ F_\theta;\theta \in \Theta \}$$ 


Where $ \Theta \subseteq R^d $ is a parameter space, so the parametric hypothesis is admitted.

$$F(\theta; \alpha,\beta) = \frac{1}{1 +(\frac{\theta}{\alpha})^\beta} =  F(\theta'; \alpha,\beta) = \frac{1}{1 +(\frac{\theta'}{\alpha})^\beta} \iff \theta = \theta' $$
So the identifiability is admitted. 

There exist well a measure $\mu$ on the Borel sets of $R^m$ s.t $F_\theta$ admits a density $f(x;\theta)$ w.r.t $\mu$. In our case $\mu$ is the Lebesgue measure on $R^p$ because we are in the continuous case.

<br>
<br>
<br>

## Log Likelihood
<br>
<br>

$$MLE(x_i) = \prod_{i=1}^{n}  \frac{(\frac{\beta}{\alpha})(\frac{X_i}{\alpha})^{\beta-1}}{[1+(\frac{X_i}{\alpha})^\beta]^2} $$

$$log L(Xi; \alpha, \beta) = \sum_{i=1}^{n}  log \left(\frac{(\frac{\beta}{\alpha})(\frac{Xi}{\alpha})^{\beta-1}}{[1+(\frac{Xi}{\alpha})^\beta]^2}\right)$$

$$log L(Xi; \alpha, \beta) = n log(\beta) - n\beta log(\alpha) + (\beta-1)\sum_{i=1}^{n} log(X_i) -2\sum_{i=1}^{n} log[1 + (\frac{X_i}{\alpha})^\beta] $$

<br>
<br>
<br>

## Score equations and Hessian matrix
<br>
<br>
To calculate the score equations we have to derivate the logL over $\alpha$ or $\beta$.
$$Score(\alpha) = \frac{\partial log L}{\partial \alpha} = -\frac{n\beta}{\alpha} + 2\sum_{i=1}^{n} \frac{(\frac{\beta}{X_i})(\frac{X_i}{\alpha})^{\beta - 1}}{[1 + (\frac{X_i}{\alpha})^\beta]}  $$
$$Score(\beta) = \frac{\partial log L}{\partial \beta} = \frac{n}{\beta} - nlog(\alpha) + \sum_{i=1}^{n} log(X_i) + 2\sum_{i=1}^{n} \frac{(\frac{X_i}{\alpha})^\beta log(\frac{X_i}{\alpha})}{[1 + (\frac{X_i}{\alpha})^\beta]}   $$

<br>
<br>

Hessian Matrix:


\begin{pmatrix} 
        \frac{\partial^2 log L}{\partial \alpha^2} & \frac{\partial^2 log L}{\partial \alpha \partial \beta} \\ \frac{\partial^2 log L}{\partial \beta \partial \alpha} & \frac{\partial^2 log L}{\partial \beta^2}
\end{pmatrix} 


with : $\frac{\partial^2 log L}{\partial \alpha \partial \beta} = \frac{\partial^2 log L}{\partial \beta \partial \alpha}$, we then have :

 
\begin{pmatrix} 
        \frac{n\beta}{\alpha^2} - 2\sum_{i=1}^{n} \frac{\frac{\beta^2}{X_i^2} (\frac{X_i}{\alpha})^{\beta-1}}{[1 + (\frac{X_i}{\alpha})^\beta]^2}&  -\frac{n}{\alpha} + 2\sum_{i=1}^{n} \frac{X_i^\beta log(\frac{X_i}{\alpha})}{[1 + (\frac{X_i}{\alpha})^\beta]^2} \\ -\frac{n}{\alpha} + 2\sum_{i=1}^{n} \frac{X_i^\beta log(\frac{X_i}{\alpha})}{[1 + (\frac{X_i}{\alpha})^\beta]^2} & -\frac{n}{\beta^2} - 2\sum_{i=1}^{n} \frac{X_i^\beta log(\frac{X_i}{\alpha})}{[1 + (\frac{X_i}{\alpha})^\beta]^2}
\end{pmatrix} 


<br>
<br>
<br>

## Newton Raphson algorithm

<br>
<br>
Goal: find the root (x axis intersection) of a function, works for real and complex functions

1. pick a random point x of the function ("first guess")
2. find the tangent to the curve at that point x
3. find the intersection of the tangent with the x axis and make it your new x
4. repeat until you're close enough (delta-y below a certain threshold which defines that you're on a point)


Input: Function f(x), initial guess x0, tolerance epsilon, maximum number of iterations max\_iterations

1. Set $x = x_0$
2. Set iteration = 0
3. Repeat the following steps until convergence or reaching the maximum number of iterations:
      1. Set $f\_x = f(x)$        
      2. Set $f\_prime\_x$ = derivative of $f(x)$
      3. Set $delta\_x = f\_x / f\_prime\_x$
      4. Set $x = x - delta\_x$
      5. Increment iteration by 1
      6. If abs($delta\_x$) < epsilon or iteration >= $max\_iterations$, exit the loop
4. Output the final approximation x as the root of the function f(x)
<br>
<br>
<br>
## Newton Raphson algorithm on R
<br>
<br>
Preparation (dependencies & data):
```{r warning=FALSE}
library(fitdistrplus)
gpigs <- read.table("surv.gpigs.txt", header = T, sep = ";")
gpigs.noncensored <- gpigs[gpigs$censored == 0,]
gpigs.mc <- gpigs.noncensored$lifetime[gpigs.noncensored$regime == "M_C"]
gpigs.mc.with_censored <- gpigs$lifetime[gpigs$regime == "M_C"]
gpigs.m43 <- gpigs.noncensored$lifetime[gpigs.noncensored$regime == "M_4.3"]


```


<br>
<br>

```{r}
score_alpha <- function(x, alpha, beta) {
  n <- length(x)
  score <- -((n * beta) / alpha) + 2 * sum((beta / x) * ((x / alpha)^(beta - 1)) / (1 + (x / alpha)^beta))
  return(score)
}

score_beta <- function(x, alpha, beta) {
  n <- length(x)
  score <- (n / beta) - (n * log(alpha)) + sum(log(x)) + 2 * sum(((x / alpha)^beta) * log(x / alpha) / (1 + (x / alpha)^beta))
  return(score)
}

loglogistic_hessian <- function(x, n, alpha, beta) {
  hessian <- matrix(0, nrow = 2, ncol = 2)
  xi <- x
  hessian[1, 1] <- (n * beta / alpha^2) - (2 * sum((beta^2 / xi^2) * ((xi / alpha)^(beta - 1)) / ((1 + (xi / alpha)^beta)^2)))
  hessian[2, 2] <- -(n / beta^2) - (2 * sum(- (xi^beta * log(xi / alpha)) / ((1 + (xi / alpha)^beta)^2)))
  hessian[2, 1] <- -(n / alpha) + (2 * sum((2 * xi^beta * log(xi / alpha)) / ((1 + (xi / alpha)^beta)^2)))
  hessian[1, 2] <- sum((2 * xi^beta * log(xi / alpha)) / ((1 + (xi / alpha)^beta)^2))
  return(hessian)
}


NR_fit_LogLogistic_hessian <- function(x, params_0, eps = 0.00001)
{
  params <- params_0
  n <- length(x)
  
  sumlogx <- sum(log(x))
  
  diff <- Inf
  alpha <- params_0[1]
  beta <- params_0[2]
  
  while (diff > eps)
  {
    params.old <- params
    # Gradient vector
    s <- c(score_alpha(x, alpha, beta), score_beta(x, alpha, beta))
    
    # Calculate Hessian matrix
    H <- loglogistic_hessian(x, n, alpha, beta)
    
    # Update parametres using hessian matrix
    params <- params - solve(H) %*% s
    alpha <- params[1]
    beta <- params[2]
    
    diff <- abs(sum(params - params.old))
  }
  
  list(params = params, H = H)
}

NR_fit_LogLogistic_hessian(gpigs.m43, c(153, 3))
```
<br>
<br>
<br>

## Monte-Carlo experiment

<br>
<br>
```{r}

res <- read.table("mc_res_with_censoring_1k_iterations_452_2.csv",header=T)
res
```

```{r warning=FALSE}

rloglogis <- function(n, alpha, beta) {
  u <- runif(n)  # Generate n random numbers from a uniform distribution
  
  x <- alpha * ((1 / u - 1)^(-1 / beta))  # Transform the uniform random numbers using the inverse CDF
  
  return(x)
}

set.seed(42)
#####################################
# some values needed for later
n_values <- c(100, 500, 1000, 5000, 10000) # sample sizes
I <- 1000 # no. of iterations
alpha0 <- 452.6
beta0 <- 2.2
#####################################
# censoring function, for a given quantile sets the max value and censors everything above
censor <- function(x, p = 0.9) {
  # for example the top 10% is everything above the 0.9 quantile
  top_ten <- quantile(x, p)
  res = x
  # censor that shit
  res[x > top_ten] = top_ten
  return(res)
}
#####################################
# the montecarlo iteration for a given sample size
mc <- function(n, I, alpha0, beta0) {
  # initialise the matrices that will track the estimates throughout the iterations
  est <- matrix(nrow = 0, ncol = 2)
  # iterate
  for (i in 1:I) {
    print(c(i, I))
    # create random data set that follows a log-logistic distribution with the given parametres
    dat <- rloglogis(n, alpha0, beta0)
    # censor the top 10%
    dat_censored <- censor(dat)
    est_temp <- NR_fit_LogLogistic_hessian(dat_censored, c(alpha0, beta0), 0.0001)$params
    est <- rbind(est, c(est_temp[1,1], est_temp[2,1]))
  }
  return(c(mean((est[,1] - alpha0)^2), mean((est[,2] - beta0)^2)))
}
# test_mc <- mc(100, I, alpha0, beta0)
#####################################
#res <- matrix(nrow = 2, ncol = 0)
#for (i in 1:length(n_values)) {
  #res <- cbind(res, mc(n_values[i], I, alpha0, beta0))
#}
#colnames(res) <- n_values[1:4]
#write.table(res, "mc_res_with_censoring_1k_iterations_452_2.csv")
#####################################
#res <- read.table("mc_res_with_censoring_1k_iterations_452_2.csv",header=T)

#barplot(log(res),
#        col = c("blue", "orange"),
#        beside = TRUE,
#        main = "Mean square error of the NR-fit parameter estimates \n based on sample size for the \n Log-Logistic distribution \n (Logarithmic scale)", xlab = "Sample size", ylab = "log(MSE)")
#legend("topright",
#       legend = c("Alpha", "Beta"),
#       fill = c("blue", "orange"))
```
<br>
<br>
<br>

# Real data analysis

<br>
<br>

## Comparison of 2 treatments with descriptive analysis

<br>
<br>
In our data we have the lifetimes of guinea pigs in days and also their regime and if the value of the lifetime is censored or not.

We want to study the resistance of guinea pigs to Tubercle Bacili.

We take for the MC guinea pigs all of them containing censored ones and non censored ones.
```{r}
summary(gpigs.mc.with_censored)
```

```{r}
summary(gpigs.m43)
```
As we can see, the guinea pigs with MC regime are surviving longer than the ones with M43 regime.

The mean for the MC guinea pigs is around 500 wich is almost three times bigger than the mean for M43 guinea pigs ($176.8$)
<br>
<br>
<br>

## fit a distribution to the "lifetime" with Exponential distribution 

<br>
<br>

### for MC group
```{r warning=FALSE}
fe <- fitdist(gpigs.mc, "exp")
fe$estimate
```
This rate is pretty low so the mean distribution will be pretty high.
Which is maybe not what we expected, The Histrogram will give more precision about it.


```{r}
denscomp(list(fe))
```
As we can see the exponential function is truly not adapted to our data.
The curve is not following well the true values.
<br>
<br>

### for M43 group 
```{r warning=FALSE}
fe <- fitdist(gpigs.m43, "exp") 
fe$estimate
```
```{r}
denscomp(list(fe))
```
As we can see the fitting is a bit better with these data but still not good enought. 
<br>
<br>
<br>

## Do the same for log logistic distribution

<br>
<br>

### Define the distribution function
```{r}
dloglogis <- function(x, alpha, beta) {
  res <- (beta/alpha) * (x/alpha)^(beta-1)*(1+(x/alpha)^beta)^-2
  return(res)
}
ploglogis <- function(q, alpha, beta) {
  res <- 1 / (1 + (q/alpha)^beta)
  return(res)
}
qloglogis <- function(p, alpha, beta) {
  res <- alpha * ((1/p) - 1)^(1/beta)
  return(res)
}
```
<br>
<br>

### Control group : 
```{r warning=FALSE}
# MC regime (without censored data for the moment)
fll <- fitdist(gpigs.mc, "loglogis", method = "mle", start=list(alpha=10, beta=5))
fll$estimate
```
```{r}
denscomp(list(fll), main = "MC regime")
```
This graph represent the lifetimes of guinea pigs for animals with MC regime but without the censored data.
As we can see the curve is following pretty well the data especially for the beginning.
<br>
<br>

### M4.3-Regime group :

```{r warning=FALSE}
# M4.3 regime
fll <- fitdist(gpigs.noncensored$lifetime[gpigs.noncensored$regime == "M_4.3"], "loglogis", method = "mle", start=list(alpha=10, beta=5))
fll$estimate
```
The estimation of alpha and beta are a bit different for the M43 guinea pigs than for the MC guinea pigs.

```{r warning=FALSE}
denscomp(list(fll), main = "M4.3 regime")
```
In this case the loglogistic distribution fit perfectly with the data as we can see on the graph.
<br>
<br>
<br>

## Try other distributions

<br>
<br>

###Weibull, Gamma, log-Normal fit on the control group

```{r warning=FALSE}
fw <- fitdist(gpigs.mc, "weibull")
fg <- fitdist(gpigs.mc, "gamma") 
fln <- fitdist(gpigs.mc, "lnorm") 
plot.legend <- c("Weibull", "lognormal", "gamma")
denscomp(list(fw, fln, fg), legendtext = plot.legend) 
```
```{r}
qqcomp(list(fw, fln, fg), legendtext = plot.legend) 
```
```{r}
cdfcomp(list(fw, fln, fg), legendtext = plot.legend) 
```
```{r}
ppcomp(list(fw, fln, fg), legendtext = plot.legend)
```
The three other distribution are fitting pretty well to our data, but the closest distribution is the gamma distribution.
<br>
<br>

### Weibull, Gamma, log-Normal fit on the 4.3 group

```{r warning=FALSE}
fw <- fitdist(gpigs.m43, "weibull")
fg <- fitdist(gpigs.m43, "gamma") 
fln <- fitdist(gpigs.m43, "lnorm") 
plot.legend <- c("Weibull", "lognormal", "gamma")
denscomp(list(fw, fln, fg), legendtext = plot.legend) 
```

```{r}
qqcomp(list(fw, fln, fg), legendtext = plot.legend) 
```

```{r}
cdfcomp(list(fw, fln, fg), legendtext = plot.legend) 
```

```{r}
ppcomp(list(fw, fln, fg), legendtext = plot.legend)
```
Same than previously the distributions are fitting well to the data of M43 guinea pigs and the closest is either the gamma distribution either the lognormal distribution.

<br>
<br>
We had estimated the $alpha = 152.4$ and $beta = 3.0$
```{r}
fw$estimate
```
For the weibull distribution the shape (beta) is to low and the scale much to high.

```{r}
fg$estimate
```
for the gamma distribution, the shape and the rate looks pretty good and we will confirm this at the end.

```{r}
fln$estimate
```
the meanlog and the sdlog are pretty fitting the data.

<br>
<br>
<br>

## provide 95\% basic bootstrap confidence interval

<br>
<br>
```{r, warning =FALSE}

loglogis <- function(x, alpha, beta) {
  res <- (beta/alpha) * (x/alpha)^(beta-1)*(1+(x/alpha)^beta)^-2
  return(res)
}
# and our data
data <- data.frame(x = gpigs$lifetime,
                   delta = gpigs$censored)
lifetimes <- gpigs$lifetime
# Function to calculate the statistic 
calculate_statistic <- function(fitdist) {
  fit_R = fitdist(data = lifetimes, distr = "loglogis", method = "mle", start=list(alpha=10, beta=5))
  return(fit_R)
}
fit_R = fitdist(data = lifetimes, distr = "loglogis", method = "mle", start=list(alpha=10, beta=5))

# Parameters
B <- 1000  # Number of bootstrap iterations
bootstrap_samples <- vector("numeric", B)

# Perform bootstrap iterations
for (i in 1:B) {
  # Randomly sample with replacement the pairs (xi, delta_i)
  bootstrap_sample <- data[sample(nrow(data), replace = TRUE), ]
  
  # Perform the desired analysis or calculation on the bootstrap sample
  bootstrap_statistic <- calculate_statistic(bootstrap_sample)
  
  # Store the result in the list of bootstrap samples
  bootstrap_samples[i] <- bootstrap_statistic
}

#display the results
npboot_CI = bootdist(fit_R ,bootmethod = "nonparam",niter = B)
summary(npboot_CI)
    
```


<br>
<br>
<br>

## Use proper information criteria and goodness-of-fit plots to discuss the fit to the data.

<br>
<br>

```{r warning = FALSE}
fw <- fitdist(gpigs.mc, "weibull")
fg <- fitdist(gpigs.mc, "gamma") 
fln <- fitdist(gpigs.mc, "lnorm") 
fe <- fitdist(gpigs.mc, "exp") 
fll <- fitdist(gpigs.mc, "loglogis", method = "mle", start=list(alpha=10, beta=5))

model_list <- list(fe, fll, fw, fg, fln)

# Noms des modèles
model_names <- c("exp", "loglogis", "weibull", "gamma","lnorm")

# Évaluer la qualité d'ajustement avec gofstat()
gof_stats <- gofstat(model_list, fitnames = model_names)

# Afficher les résultats
print(gof_stats)

```
The most favorized distribution for the $mc$ guinea pigs is the gamma one closely followed by the weibull one.
<br>
<br>

```{r warning=FALSE}

fw <- fitdist(gpigs.m43, "weibull")
fg <- fitdist(gpigs.m43, "gamma") 
fln <- fitdist(gpigs.m43, "lnorm") 
fe <- fitdist(gpigs.m43, "exp") 
fll <- fitdist(gpigs.m43, "loglogis", method = "mle", start=list(alpha=10, beta=5))

model_list <- list(fe, fll, fw, fg, fln)

# Noms des modèles
model_names <- c("exp", "loglogis", "weibull", "gamma","lnorm")

# Évaluer la qualité d'ajustement avec gofstat()
gof_stats <- gofstat(model_list, fitnames = model_names)

# Afficher les résultats
print(gof_stats)

```

In the case of $m43$ guinea pigs the most favorized distribution is the gamma distribution for the Goodness-of-fit statistics
and the loglogistic for the Goodness-of-fit criteria.